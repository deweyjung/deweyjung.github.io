---
title: 용어
date: 2019-09-17 18:25:14
tags: AI
categories:
    - 00. AI
    - 00. 용어
---
* 편미분 : 부분미분, 기울기, 최소 오차 찾기 위해 사용
* 역전파 : backward propagation
* 순전파 : forward propagation
* 시그모이드(sigmoid) : 1 / (1 + (exp(-x)))
* 소프트맥스(Softmax) : 입력값에 대한 출력값을 부드럽게 조정해 주는 함수  
소프트맥스의 총합은 1  
yk = exp(ak) / ∑(i=1, n) exp(ai) → 너무 큰값을 대입해서 오버플로우(overflow)가 발생하지 않도록 수식을 수정 → yk = exp(ak + C') / ∑(i=1, n) exp(ai + C')   
일반적으로 C'는 입력값 중 최대값 사용
* ReLU : 입력값이 특정값 이상일 경우 출력. 그외에는 출력하지 않음(0)
* Affine : 신경망 순전파에서 사용하는 행렬의 곱
* 스칼라 값 : 정해진 특정한 값
* 평균 제곱 오차(mean squared errror, MSE) : 손실 함수 중 하나. 가장 많이 쓰이는 함수  
E = (1/2)∑(k)(yk - tk)^2
* 교차 엔트로피 오차(cross entropy error, CEE) : 손실 함수 중 하나  
E = ∑(k)(tk * logyk)
* 항등함수(identity function) : 입력을 그대로 출력하는 함수
* 오차역전파법(back propagation) : 수식의 매개변수에 따른 기울기를 구해 오차를 줄이는 방법. 수치 미분에 비해 효율적이고 빠름. 매개변수가 많아도 효율적으로 계산할 수 있음.
* 확률적 경사 하강법(stochastic gradient descent) : 기울기에 따라 방향을 정해서 움직이는 방법. 방향에 따른 기울기의 변화에서는 비효율적임.
* 모멘텀(momentum) : 기울기 방향으로 물체가 가속됨을 적용한 방법. 매개변수에 속도를 계속 더해줌. 속도는 알파 곱하기 속도에 학습률 곱하기 기울기로 계산함.
* AdaGrad : 학습률을 점차 감소시키는 방법. 매개변수의 원소마다 적응적으로 갱신 정도를 조정. 기울기를 제곱하고 그것을 더해 1분의 루트로 감쌈.
* Adam : momentum과 AdaGrad를 결합한 방법.
* 코딩시 가장 작은값 : 1e-7
* 학습률 : 한번의 학습으로 얼마만큼 학습할 지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것.
* 가중치 : weight. 매개변수.
* Xavier 초기값 : sigmoid함수, tanh함수등의 s자 모양 곡선일 경우는 좌우 대칭이랑 중앙 부근이 선형인 함수임. xavier초기값이 적당.
* He 초깃값 : ReLU에 적당.
* 오버피팅 : 한 데이터 셋에만 지나치게 최적화 된 상태. 다른 데이터 셋에서 정확성이 떨어지게 됨. 매개변수가 많고 표현력이 높은 모델, 훈련 데이터가 적은 경우 발생.
* 표준편차 : 평균에서 자료의 값이 많이 떨어질수록 표준편차가 크다.
* 배치 정규화 알고리즘(batch normalization algorithm) : 각층이 활성화를 적당히 퍼뜨리도록 강제. 학습이 원활하게 수행됨. 학습속도개선, 초깃값에 크게 의존하지 않음, 오버피팅 억제.
* 에폭(epoch) : 하나의 단위로서 모든 훈련을 소진했을 경우 훈련 횟수.
* 오버피팅 억제 방법 :
1. 손실함수에 가중치의 L2노름을 더함.
2. 드롭아웃 : 학습시 임의의 노드를 골라 삭제. 단 시험 때는 각 뉴런의 출력에 훈련때 삭제한 비율을 곱하여 출력.
* 하이퍼파라미터 : 각 층의 뉴런수, 배치 크기, 매개변수 갱신 시의 학습률과 가중치 감소 등.
* 데이터 종류  
훈련데이터 : 매개변수 학습  
검증데이터 : 하이퍼파라미터 학습  
시험데이터 : 범용 성능시험
* 하이퍼파라미터 최적화 단계
1. 하이퍼파라미터 값의 범위를 설정
2. 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출
3. 2단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습, 검증데이터로 정확도 평가(단, 에폭은 작게 설정)
4. 2,3 단계를 특정 횟수 반복하며, 결과를 보고 하이퍼파라미터의 범위를 좁힘.
* 전이학습(tranfer learning) : 이미 학습된 가중치를 다른 신경망에 복사하여 재사용하는 방법
